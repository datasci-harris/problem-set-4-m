---
title: "Pset 4"
format: html
---

Partner 1: Mahnoor
Partner 2: Mauricio Paniagua

Import packages
```{python}
import os
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import time
import matplotlib.pyplot as plt


```

Download and explore the Provider of Services (POS) file (Partner 1)

```{python}
#Q1
import pandas as pd
import altair as alt

# Load the 2016 dataset
file_path = '/mnt/data/pos2016.csv'
pos2016 = pd.read_csv("C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\problem-set-4-m\\pos2016.csv")


# Display the first few rows of the dataset
pos2016.head()
```

#####Q1
#The variables pulled are 
PRVDR_CTGRY_SBTYP_CD: ROVIDER Category (Short Term)
PRVDR_CTGRY_CD: PROVIDER Category (Long Term)
FAC_NAME: Facility Name
PRVDR_NUM: unique CMS certification number
STATE_CD: State Abbreviation
PGM_TRMNTN_CD: the current termination status for the
 provider
ZIP_CD: ZIP CODE 

```{python}
##Q2 (a)
# Subset data for short-term hospitals
short_term_hospitals_2016 = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]

# Get the number of hospitals
num_hospitals_2016 = short_term_hospitals_2016['PRVDR_NUM'].nunique()
print(f"Number of short-term hospitals in 2016: {num_hospitals_2016}")
#Number of short-term hospitals in 2016: 7245
#Yes, there are 7245 hospitals categorised as short term facilities

##Q2 (b) According to NHS, short term hospitals 
# in 2016 were 3,436 in number. This is a fairly
#  low number compared to
#CMS data. This discrepancy could be due to 
# categorisation of what constitutes a short term hospital 
# and if they were eligble for medicaid/medicare.
```


```{python}
#Q3
# Specify the file paths
file_paths = [
    "C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\problem-set-4-m\\pos2017.csv",
    "C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\problem-set-4-m\\pos2018.csv",
    "C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\problem-set-4-m\\pos2019.csv"
]

data_frames = []

# Loop through file paths and process each year's data
for year, file_path in zip([2017, 2018, 2019], file_paths):
    # Use 'ISO-8859-1' encoding to avoid UnicodeDecodeError
    df = pd.read_csv(file_path, encoding='ISO-8859-1')
    
    # Filter for short-term hospitals
    short_term_df = df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1)].copy()
    
    # Assign the 'Year' column
    short_term_df['Year'] = year
    data_frames.append(short_term_df)

# Combine all years into a single DataFrame
combined_data = pd.concat([short_term_hospitals_2016] + data_frames, ignore_index=True)

# Replace NaN values in 'Year' column with 2016
combined_data['Year'].fillna(2016, inplace=True)
combined_data['Year'] = combined_data['Year'].astype(int)
# Display the first few rows to confirm
combined_data.head()

```


```{python}
#Q3
# Display the first few rows
combined_data.head()
```

```{python}
#Q3
# Save to a CSV file for external viewing
combined_data.to_csv('combined_data_full.csv', index=False)
```

```{python}
#Q3
import altair as alt

# Count the number of observations per year
observations_by_year = combined_data.groupby('Year').size().reset_index(name='Observations')

# Create the bar chart
observations_plot = alt.Chart(observations_by_year).mark_bar().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Observations:Q', title='Number of Observations', scale=alt.Scale(domain=[0, observations_by_year['Observations'].max() + 10])),  # Adjust the y-axis scale as needed
    color=alt.Color('Year:O', legend=alt.Legend(title="Year"))  # Color encoding by Year
).properties(
    title='Number of Observations by Year'
)

# Create text labels for each bar with reduced font size
text_labels = observations_plot.mark_text(
    align='center',
    baseline='bottom',
    dy=-5,  # Adjust the vertical position of the text
    fontSize=8  # Set the desired font size for the text
).encode(
    x=alt.X('Year:O', title='Year'),  # Ensure the x encoding matches the bars
    y=alt.Y('Observations:Q', title='Number of Observations'),  # Ensure the y encoding matches the bars
    text='Observations:Q'  # Display the Observations value
)

# Combine the bar chart and text labels
final_plot = observations_plot + text_labels

final_plot.display()

```


```{python}
#Q4
# Count unique hospitals per year
unique_hospitals_by_year = combined_data.groupby('Year')['PRVDR_NUM'].nunique().reset_index(name='Unique_Hospitals')

# Create the bar chart for unique hospitals
unique_hospitals_plot = alt.Chart(unique_hospitals_by_year).mark_bar().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Unique_Hospitals:Q', title='Number of Unique Hospitals', scale=alt.Scale(domain=[0, unique_hospitals_by_year['Unique_Hospitals'].max() + 10])),  # Adjust the y-axis scale as needed
    color=alt.Color('Year:O', legend=alt.Legend(title="Year"))  # Color encoding by Year
).properties(
    title='Number of Unique Hospitals by Year'
)

# Create text labels for each bar with reduced font size
unique_hospital_text_labels = unique_hospitals_plot.mark_text(
    align='center',
    baseline='bottom',
    dy=-5,  # Adjust the vertical position of the text
    fontSize=7  # Set the desired font size for the text
).encode(
    x=alt.X('Year:O', title='Year'),  # Ensure the x encoding matches the bars
    y=alt.Y('Unique_Hospitals:Q', title='Number of Unique Hospitals'),  # Ensure the y encoding matches the bars
    text='Unique_Hospitals:Q'  # Display the Unique Hospitals value
)

# Combine the bar chart and text labels
unique_hospitals_final_plot = unique_hospitals_plot + unique_hospital_text_labels

unique_hospitals_final_plot.display()
```

##Q4
Since both the number of observations and the number of unique hospitals plot 
show similar trends, it indicates that the dataset is relatively stable, with few 
hospitals entering or exiting the dataset each year. This consistency suggests that 
while some hospitals may close or change ownership, the overall number of operational 
short-term hospitals remains steady.


Part 2: Identify hospital closures in POS file (Partner 2)
Q1
```{python}

# Define paths

file_path_2016 = r"C:\Users\arifm\OneDrive\Documents\GitHub\problem-set-4-m\pos2016.csv"

file_path_2017 = r"C:\Users\arifm\OneDrive\Documents\GitHub\problem-set-4-m\pos2017.csv"

file_path_2018 = r"C:\Users\arifm\OneDrive\Documents\GitHub\problem-set-4-m\pos2018.csv"

file_path_2019 = r"C:\Users\arifm\OneDrive\Documents\GitHub\problem-set-4-m\pos2019.csv"

# Read in provided paths

df_2016 = pd.read_csv(file_path_2016, encoding="ISO-8859-1")

df_2017= pd.read_csv(file_path_2017, encoding="ISO-8859-1")

df_2018= pd.read_csv(file_path_2018, encoding="ISO-8859-1")

df_2019= pd.read_csv(file_path_2019,  encoding="ISO-8859-1")
```
```{python}
# Filter active hospitals in 2016
active_hospitals = df_2016[df_2016['PGM_TRMNTN_CD'] == 0][['FAC_NAME', 'ZIP_CD']].copy()
active_hospitals['Suspected Closure Year'] = None

# Define a function to check active status in a given year
def check_hospitals_status(df, year, active_hospitals):
    # Filter active providers for the year
    active_in_year = set(df[df['PGM_TRMNTN_CD'] == 0]['FAC_NAME'])
    
    # Check for hospitals missing or inactive in the year
    for index, row in active_hospitals.iterrows():
        facility_name = row['FAC_NAME']
        
        # Mark the year of suspected closure if the hospital is no longer active
        if facility_name not in active_in_year and pd.isna(active_hospitals.at[index, 'Suspected Closure Year']):
            active_hospitals.at[index, 'Suspected Closure Year'] = year

# Apply the function across the years 2017 to 2019
check_hospitals_status(df_2017, 2017, active_hospitals)
check_hospitals_status(df_2018, 2018, active_hospitals)
check_hospitals_status(df_2019, 2019, active_hospitals)

# Filter hospitals suspected of closure
closed_hospitals = active_hospitals.dropna(subset=['Suspected Closure Year'])

# Display results
print(f"Number of suspected closed hospitals: {len(closed_hospitals)}")
print(closed_hospitals[['FAC_NAME', 'ZIP_CD', 'Suspected Closure Year']])

    

```

 Question 2

```{python}
# Sort by hospital name and select the first 10 rows
sorted_closed_hospitals = closed_hospitals.sort_values(by='FAC_NAME').head(10)

# Display the hospital names and suspected closure year
print(sorted_closed_hospitals[['FAC_NAME', 'Suspected Closure Year']])

```

 Question 3

```{python}
# Step 1: Identify potential mergers/acquisitions
# Function to check if the count of active hospitals in a zip code decreased the year after closure
def filter_mergers(closed_hospitals, df_by_year):
    potential_mergers = []

    # Iterate over each suspected closed hospital
    for index, row in closed_hospitals.iterrows():
        zip_code = row['ZIP_CD']
        closure_year = int(row['Suspected Closure Year'])
        
        # Check if we have data for the following year
        next_year_df = df_by_year.get(closure_year + 1)
        
        if next_year_df is not None:
            # Count active hospitals in this zip code in the suspected closure year and the year after
            active_in_year = df_by_year[closure_year][(df_by_year[closure_year]['ZIP_CD'] == zip_code) &
                                                      (df_by_year[closure_year]['PGM_TRMNTN_CD'] == '0')]
            active_next_year = next_year_df[(next_year_df['ZIP_CD'] == zip_code) &
                                            (next_year_df['PGM_TRMNTN_CD'] == '0')]

            # If the count of active hospitals did not decrease, it may be a merger/acquisition
            if len(active_in_year) <= len(active_next_year):
                potential_mergers.append(index)
                
    # Filter out potential mergers from suspected closures
    corrected_closed_hospitals = closed_hospitals.drop(index=potential_mergers)
    
    return corrected_closed_hospitals, closed_hospitals.loc[potential_mergers]

# Create a dictionary with data for each year for easier lookup
df_by_year = {2016: df_2016, 2017: df_2017, 2018: df_2018, 2019: df_2019}

# Step 2: Apply the filter to identify potential mergers and obtain the corrected list of closures
corrected_closed_hospitals, potential_mergers = filter_mergers(closed_hospitals, df_by_year)

# Step 3: Reporting
print(f"Number of hospitals potentially involved in mergers/acquisitions: {len(potential_mergers)}")
print(f"Number of corrected suspected closures after filtering: {len(corrected_closed_hospitals)}")

# Step 4: Sort and display the corrected list of closures by name, showing the first 10 rows
sorted_corrected_closed_hospitals = corrected_closed_hospitals.sort_values(by='FAC_NAME').head(10)
print(sorted_corrected_closed_hospitals[['FAC_NAME', 'ZIP_CD', 'Suspected Closure Year']])

```

Download Census zip code shapefile (Partner 1)


.shp – The main shapefile that stores the geometry of the spatial features (e.g., points, lines, polygons).
.shx – The shape index format file that stores index data for quickly accessing the geometry in the .shp file. The file size is 514218 KB.
.dbf – A database file that stores attribute data associated with the shapes in a tabular format (like a spreadsheet). It contains fields for each shape's attributes (e.g., name, type, ID). The file size is 429 KB.
.prj – The projection file that defines the coordinate system and projection information for the shapefile. The file size is 1 KB.
.xml – An XML file that can store metadata about the shapefile, including data source, purpose, date created, and additional information for reference. The file size is 5 KB.

##The .shp (shape file) is the heaviest among all the files: 514,218 KB.

##The .prj (projection file) is the lightest: 1 KB


```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
```


```{python}
#Q1
zip_shapefile = gpd.read_file("C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\problem-set-4-m\\gz_2010_us_860_00_500k.shp")
```


```{python}
#Q1
# Step 1: Load the shapefile and rename ZCTA5 to ZIP_CD
zip_shapefile = gpd.read_file("C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\problem-set-4-m\\gz_2010_us_860_00_500k.shp")
zip_shapefile = zip_shapefile.rename(columns={'ZCTA5': 'ZIP_CD'})

# Ensure ZIP_CD is a string for consistency
zip_shapefile['ZIP_CD'] = zip_shapefile['ZIP_CD'].astype(str)

# Step 2: Filter for Texas ZIP Codes (prefixes 733, 75, 76, 77, 78)
texas_zip_shapefile = zip_shapefile[zip_shapefile['ZIP_CD'].str.startswith(('733', '75', '76', '77', '78', '79'))]

# Step 3: Load the cleaned POS dataset and ensure ZIP_CD is string and clean up any decimal formatting
combined_data_full = pd.read_csv("C:\\Users\\arifm\\OneDrive\\Documents\\GitHub\\problem-set-4-m\\combined_data_full.csv")
combined_data_full['ZIP_CD'] = combined_data_full['ZIP_CD'].astype(str).str.split('.').str[0].str.zfill(5)

# Filter the POS data for Texas ZIP codes
texas_hospitals = combined_data_full[combined_data_full['ZIP_CD'].str.startswith(('733', '75', '76', '77', '78', '79'))]

# Step 4: Count hospitals per ZIP Code in Texas
hospitals_per_zip = texas_hospitals.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# Step 5: Merge the hospital counts with the Texas ZIP Code shapefile
texas_zip_shapefile = texas_zip_shapefile.merge(hospitals_per_zip, on='ZIP_CD', how='left').fillna(0)

# Step 6: Display the result for verification
print(texas_zip_shapefile[['ZIP_CD', 'hospital_count']].head())
```


```{python}
#Q2
# Plot a choropleth of the number of hospitals per ZIP code in Texas
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
texas_zip_shapefile.plot(column='hospital_count', 
                         cmap='Reds', 
                         linewidth=0.8, 
                         ax=ax, 
                         edgecolor='0.8', 
                         legend=True)

ax.set_title("Number of Hospitals per ZIP Code in Texas (2016)")
plt.show()
```



Calculate zip code’s distance to the nearest hospital (Partner 2)


```{python}

import geopandas as gpd

# Load the shapefile
zip_shapefile = gpd.read_file(r"C:\Users\arifm\OneDrive\Documents\GitHub\problem-set-4-m\gz_2010_us_860_00_500k.shp")

zip_shapefile = zip_shapefile.rename(columns={'ZCTA5': 'ZIP_CD'})

# Ensure ZIP_CD is a string for consistency
zip_shapefile['ZIP_CD'] = zip_shapefile['ZIP_CD'].astype(str)

# Step 2: Filter for Texas ZIP Codes (prefixes 75, 76, 77, 78)
texas_zip_shapefile = zip_shapefile[zip_shapefile['ZIP_CD'].str.startswith(('75', '76', '77', '78'))]

# Step 3: Load the cleaned POS dataset and ensure ZIP_CD is string and clean up any decimal formatting
combined_data_full = pd.read_csv(r"C:\Users\arifm\OneDrive\Documents\GitHub\problem-set-4-m\combined_data_full.csv")
combined_data_full['ZIP_CD'] = combined_data_full['ZIP_CD'].astype(str).str.split('.').str[0].str.zfill(5)

# Filter the POS data for Texas ZIP codes
texas_hospitals = combined_data_full[combined_data_full['ZIP_CD'].str.startswith(('75', '76', '77', '78'))]

# Step 4: Count hospitals per ZIP Code in Texas
hospitals_per_zip = texas_hospitals.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# Step 5: Merge the hospital counts with the Texas ZIP Code shapefile
texas_zip_shapefile = texas_zip_shapefile.merge(hospitals_per_zip, on='ZIP_CD', how='left').fillna({'hospital_count': 0})

# Step 6: Display the result for verification
print(texas_zip_shapefile[['ZIP_CD', 'hospital_count']].head())

```

Part D, Question 1

```{python}
# If `zip_shapefile` contains polygons, you can calculate centroids like this:
zips_data = zip_shapefile.copy()  # Or use an appropriate subset of zip_shapefile
zips_data['centroid'] = zips_data.geometry.centroid
zips_data['longitude'] = zips_data['centroid'].x
zips_data['latitude'] = zips_data['centroid'].y

# Create a GeoDataFrame for ZIP code centroids using longitude and latitude columns
zips_all_centroids = gpd.GeoDataFrame(
    zips_data, 
    geometry=gpd.points_from_xy(zips_data.longitude, zips_data.latitude),
    crs="EPSG:4326"
)


print(f"Dimensions of zips_all_centroids: {zips_all_centroids.shape}")
print(zips_all_centroids.head())


```

Part D, Question 2:

```{python}
# Filter Texas ZIP codes
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZIP_CD'].str.startswith(('75', '76', '77', '78'))]

# Filter Texas and bordering states ZIP codes
bordering_prefixes = ('75', '76', '77', '78', '87', '88', '73', '74', '71', '72', '70')
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZIP_CD'].str.startswith(bordering_prefixes)]

# Count unique ZIP codes in Texas and Texas + bordering states
num_texas_zipcodes = zips_texas_centroids['ZIP_CD'].nunique()
num_borderstates_zipcodes = zips_texas_borderstates_centroids['ZIP_CD'].nunique()

print(f"Number of unique Texas ZIP codes: {num_texas_zipcodes}")
print(f"Number of unique Texas and bordering states ZIP codes: {num_borderstates_zipcodes}")

# Define the function to check if two polygons intersect
def check_polygons_intersect(polygon1, polygon2):
    return polygon1.intersects(polygon2)

# Combine all Texas ZIP code geometries into a single polygon
texas_polygon = zips_texas_centroids.geometry.union_all()

# Combine all bordering states ZIP code geometries into a single polygon
bordering_state_polygon = zips_texas_borderstates_centroids.geometry.union_all()

# Check if Texas polygon intersects the bordering states polygon
is_bordering = check_polygons_intersect(texas_polygon, bordering_state_polygon)
print("Does Texas border the combined polygon of bordering states?", is_bordering)
```

Part D, Question 3

```{python}

# Step 1: Create hospital_count by counting hospitals per ZIP code in 2016
# Assuming combined_data_full has a 'ZIP_CD' column and only contains records for 2016
hospitals_per_zip_2016 = combined_data_full.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# Filter for ZIP codes with at least one hospital
hospitals_2016 = hospitals_per_zip_2016[hospitals_per_zip_2016['hospital_count'] > 0][['ZIP_CD']]

# Step 2: Merge zips_texas_borderstates_centroids with hospitals_2016 on ZIP_CD
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospitals_2016,
    on='ZIP_CD',
    how='inner'  # Inner join to keep only ZIP codes with hospitals in 2016
)

# Display the result and check number of ZIP codes with hospitals
print(zips_withhospital_centroids.head())
print(f"Number of ZIP codes with at least one hospital in 2016: {len(zips_withhospital_centroids)}")

```

Part D, Question 4

```{python}

# Assuming zips_texas_centroids and zips_withhospital_centroids are already defined

# Step A: Subset to 10 ZIP codes
zips_texas_subset = zips_texas_centroids.head(10)

# Measure time for the subset calculation
start_time = time.time()

# Reproject both GeoDataFrames to a projected CRS
zips_texas_subset = zips_texas_subset.to_crs(epsg=32614)  # UTM zone for Texas (adjust EPSG if needed)
zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=32614)

# Calculate distance to nearest hospital for the subset
zips_texas_subset['nearest_hospital_distance'] = zips_texas_subset.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)

end_time = time.time()
time_taken_subset = end_time - start_time
print(f"Time taken for subset calculation: {time_taken_subset:.4f} seconds")

# Estimate for full calculation (this is a rough estimate, adjust based on experience)
estimated_full_time = time_taken_subset * (len(zips_texas_centroids) / len(zips_texas_subset))
print(f"Estimated time for full calculation: {estimated_full_time:.4f} seconds")

# Step B: Full calculation
start_time_full = time.time()

# Reproject the full GeoDataFrames
zips_texas_centroids = zips_texas_centroids.to_crs(epsg=32614)  # UTM zone for Texas (adjust EPSG if needed)

# Calculate distance to nearest hospital for all Texas ZIP codes
zips_texas_centroids['nearest_hospital_distance'] = zips_texas_centroids.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)

end_time_full = time.time()
time_taken_full = end_time_full - start_time_full
print(f"Time taken for full calculation: {time_taken_full:.4f} seconds")
print(f"Estimated vs Actual: {estimated_full_time:.4f} vs {time_taken_full:.4f} seconds")

# Step C: Check the .prj file
# Assuming the shapefile for Texas ZIP codes is available
prj_file_path = r"C:\Users\arifm\OneDrive\Documents\GitHub\problem-set-4-m\gz_2010_us_860_00_500k.prj"
with open(prj_file_path, 'r') as prj_file:
    crs_info = prj_file.read()

print("CRS Information:")
print(crs_info)

# The unit is typically indicated in the .prj file; you might see "meters" for most UTM projections.
# If it’s in meters, convert distances
# Conversion factor: 1 meter = 0.000621371 miles

# If the CRS is in meters, convert distances
if "meters" in crs_info.lower():
    zips_texas_centroids['nearest_hospital_distance_miles'] = zips_texas_centroids['nearest_hospital_distance'] * 0.000621371
    print(zips_texas_centroids[['ZIP_CD', 'nearest_hospital_distance_miles']].head())
```

Part D, Question 5

```{python}
# Step 1: Calculate the distance to the nearest hospital for each ZIP code in Texas
# First, ensure that zips_texas_centroids and zips_withhospital_centroids are defined

# Create a new column for distance to nearest hospital
zips_texas_centroids['nearest_hospital_distance'] = zips_texas_centroids.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)

# Step 2: Calculate average distance to the nearest hospital for each ZIP code
average_distances = zips_texas_centroids[['ZIP_CD', 'nearest_hospital_distance']].copy()

# Group by ZIP code and calculate the average distance
avg_distance_by_zip = average_distances.groupby('ZIP_CD')['nearest_hospital_distance'].mean().reset_index()

# Step 3: Convert from meters to miles
# Assuming the distance is in meters
avg_distance_by_zip['avg_distance_to_hospital_miles'] = avg_distance_by_zip['nearest_hospital_distance'] / 1609.34

# Step 4: Report the average distance in miles
print("Average distance to nearest hospital in miles:")
print(avg_distance_by_zip[['ZIP_CD', 'avg_distance_to_hospital_miles']])

# Step 5: Merge with original GeoDataFrame for mapping
zips_map = zips_texas_centroids.merge(avg_distance_by_zip, on='ZIP_CD')

# Step 6: Map the values
fig, ax = plt.subplots(1, 1, figsize=(12, 10))
zips_map.boundary.plot(ax=ax, linewidth=1)
zips_map.plot(column='avg_distance_to_hospital_miles', ax=ax, legend=True,
               legend_kwds={'label': "Average Distance to Nearest Hospital (Miles)", 'orientation': "horizontal"},
               cmap='OrRd', missing_kwds={'color': 'lightgrey'})
plt.title('Average Distance to Nearest Hospital by ZIP Code in Texas')
plt.show()
```

#Effects of closures on access in Texas (Partner 1)

```{python}
#Q1
# Load CSV files with specified encoding to handle special characters
pos2016 = pd.read_csv('pos2016.csv', encoding='ISO-8859-1')
pos2017 = pd.read_csv('pos2017.csv', encoding='ISO-8859-1')
pos2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
pos2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')

# Filter for active Texas hospitals in 2016
active_hospitals = pos2016[(pos2016['STATE_CD'] == 'TX') & (pos2016['PGM_TRMNTN_CD'] == 0)][['FAC_NAME', 'ZIP_CD']].copy()
active_hospitals['Suspected Closure Year'] = None

# Define a function to check active status in a given year for Texas hospitals
def check_hospitals_status(df, year, active_hospitals):
    # Filter active providers for the year in Texas
    active_in_year = set(df[(df['STATE_CD'] == 'TX') & (df['PGM_TRMNTN_CD'] == 0)]['FAC_NAME'])
    
    # Check for hospitals missing or inactive in the year
    for index, row in active_hospitals.iterrows():
        facility_name = row['FAC_NAME']
        
        # Mark the year of suspected closure if the hospital is no longer active
        if facility_name not in active_in_year and pd.isna(row['Suspected Closure Year']):
            active_hospitals.at[index, 'Suspected Closure Year'] = year

# Apply the function across the years 2017 to 2019
check_hospitals_status(pos2017, 2017, active_hospitals)
check_hospitals_status(pos2018, 2018, active_hospitals)
check_hospitals_status(pos2019, 2019, active_hospitals)

# Filter hospitals suspected of closure
closed_hospitals = active_hospitals.dropna(subset=['Suspected Closure Year'])

# Group by ZIP code to get the count of closures in each ZIP code
closures_by_zipcode = closed_hospitals.groupby('ZIP_CD').size().reset_index(name='Number of Closures')

# Display the results
print("List of ZIP codes in Texas with hospital closures from 2016-2019:")
print(closures_by_zipcode)
```

```{python}
#Q2
##Merging the files
# Rename the 'Number of Closures' column in closures_by_zipcode to avoid duplicates
closures_by_zipcode = closures_by_zipcode.rename(columns={'Number of Closures': 'Closures'})

# Ensure that both ZIP_CD columns are of type string
closures_by_zipcode['ZIP_CD'] = closures_by_zipcode['ZIP_CD'].astype(str)
texas_zip_shapefile['ZIP_CD'] = texas_zip_shapefile['ZIP_CD'].astype(str)

# Remove the decimal point from ZIP_CD in closures_by_zipcode
closures_by_zipcode['ZIP_CD'] = closures_by_zipcode['ZIP_CD'].astype(str).str.split('.').str[0]
# Check the types
print("closures_by_zipcode ZIP_CD type:", closures_by_zipcode['ZIP_CD'].dtype)
print("texas_zip_shapefile ZIP_CD type:", texas_zip_shapefile['ZIP_CD'].dtype)


# Perform the merge
merged_data = texas_zip_shapefile.merge(closures_by_zipcode, on='ZIP_CD', how='left').fillna(0)

# Check if the merge was successful by displaying the first few rows
print("Merged Data Sample:")
print(merged_data.head())

# Check the columns to ensure no duplicates exist
print("\nColumns in merged DataFrame:")
print(merged_data.columns)
```

```{python}
#Q2

import matplotlib.pyplot as plt
import geopandas as gpd

# Step 6: Plot the choropleth map
fig, ax = plt.subplots(1, 1, figsize=(14, 14))

# Plot the number of closures per ZIP code
merged_data.plot(
    column='Closures',
    ax=ax,
    legend=True,
    cmap='OrRd',
    edgecolor='black',  # Outline for ZIP codes
    legend_kwds={
        'label': "Number of Hospital Closures",
        'orientation': "horizontal",
        'shrink': 0.8,  # Shrink legend to better fit below the map
    },
    missing_kwds={
        "color": "lightgrey",
        "label": "No data",
    }
)

# Add title and labels
ax.set_title('Hospital Closures in Texas by ZIP Code (2016-2019)', fontsize=18, fontweight='bold')
ax.set_axis_off()  # Hide axis borders and ticks


# Save figure
plt.savefig("texas_hospital_closures_map.png", dpi=300, bbox_inches='tight')

plt.show()

```

```{python}
#Q2
# Count the number of directly affected zip codes
# Filter for zip codes where the number of closures is greater than 0
affected_zip_codes_count = merged_data[merged_data['Closures'] > 0].shape[0]

# Print the result
print(f'There are {affected_zip_codes_count} directly affected zip codes in Texas.')
```


```{python}
#Q3


# Step 2: Create a GeoDataFrame for directly affected ZIP codes
directly_affected_zip_codes = closures_by_zipcode[closures_by_zipcode['Closures'] > 0]
directly_affected_gdf = texas_zip_shapefile[texas_zip_shapefile['ZIP_CD'].isin(directly_affected_zip_codes['ZIP_CD'])]

# Step 3: Create a 10-mile buffer around the directly affected ZIP codes
directly_affected_gdf = directly_affected_gdf.set_geometry(directly_affected_gdf.geometry)
buffered_zip_codes = directly_affected_gdf.geometry.buffer(10 * 1609.34)  # 10 miles in meters

# Create a new GeoDataFrame with the buffered geometries
buffered_gdf = gpd.GeoDataFrame(geometry=buffered_zip_codes)

# Step 4: Spatial join with the overall Texas ZIP code shapefile to find all ZIP codes within the buffer
# Use predicate instead of op
indirectly_affected_zip_codes = gpd.sjoin(texas_zip_shapefile, buffered_gdf, how='inner', predicate='intersects')

# Step 5: Count the number of indirectly affected ZIP codes
indirectly_affected_zip_codes_count = indirectly_affected_zip_codes['ZIP_CD'].nunique()

# Display the results
print(f"Number of indirectly affected ZIP codes in Texas: {indirectly_affected_zip_codes_count}")

```


```{python}
#Q4
import geopandas as gpd
import matplotlib.pyplot as plt

# Assuming indirectly_affected_zip_codes is already defined
# Create a category column in texas_zip_shapefile for plotting
texas_zip_shapefile['Category'] = 'Not Affected'  # Default category

# Mark directly affected ZIP codes
texas_zip_shapefile.loc[texas_zip_shapefile['ZIP_CD'].isin(directly_affected_zip_codes['ZIP_CD']), 'Category'] = 'Directly Affected'

# Mark indirectly affected ZIP codes (those within the buffer) that are not directly affected
indirectly_affected_zip_set = set(indirectly_affected_zip_codes['ZIP_CD'])
texas_zip_shapefile.loc[
    texas_zip_shapefile['ZIP_CD'].isin(indirectly_affected_zip_set) & 
    ~texas_zip_shapefile['ZIP_CD'].isin(directly_affected_zip_codes['ZIP_CD']), 
    'Category'
] = 'Indirectly Affected'

# Step 2: Plotting
fig, ax = plt.subplots(1, 1, figsize=(14, 10))

# Plot the ZIP codes with different colors for each category
color_map = {
    'Directly Affected': 'lightblue',
    'Indirectly Affected': 'darkblue',
    'Not Affected': 'lightgrey'
}

texas_zip_shapefile.plot(column='Category', 
                         color=texas_zip_shapefile['Category'].map(color_map),
                         edgecolor='black', 
                         ax=ax)

# Add title and legend
plt.title('Hospital Closure Impact in Texas by ZIP Code', fontsize=18, fontweight='bold')
ax.set_axis_off()  # Hide axis

#  legend
import matplotlib.patches as mpatches

handles = [mpatches.Patch(color=color_map[key], label=key) for key in color_map.keys()]
ax.legend(handles=handles, title="Closure Impact", loc='lower left')

# Save figure
plt.savefig("texas_hospital_closures_choropleth.png", dpi=300, bbox_inches='tight')

plt.show()

```

```{python}

```

##Q1 (Partner 1)
The first pass method may rely on straightforward queries or analyses,
like counting the number of closures recorded in the data or using 
flagging criteria to identify potentially affected facilities.
While it can quickly highlight areas of concern, 
it is usually limited by potential inaccuracies in the data, lack of context, 
and failure to consider nuances. Therefore, follow-up verification 
and validation processes are often necessary to ensure 
the findings are accurate and comprehensive, as the initial analysis 
may overlook critical factors influencing the actual situation.

##Question 2 (Partner 2)

Identifying ZIP codes affected by hospital closures provides insight into healthcare access but has significant limitations. Closures can be misleading due to hospital mergers, where services continue under different names, obscuring the true impact on access. Additionally, the variation in ZIP code size and population density means that a closure in a rural area may not affect access as severely as one in a densely populated urban environment. A static analysis that counts open hospitals neglects shifts in healthcare needs and service offerings, while transportation barriers can further limit access for many residents. To improve this measure, a dynamic analysis could track changes over time, integrating transportation data to assess travel times and public transit routes. Including socioeconomic factors and community health assessments would provide a more nuanced understanding of access issues, while considering alternative healthcare options like urgent care and telemedicine could broaden the perspective on healthcare availability. Collaborating with local health organizations can yield valuable insights into the community impacts of closures and enhance our understanding of access disparities.



